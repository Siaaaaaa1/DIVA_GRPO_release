# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
import os
from collections import defaultdict
from io import BytesIO
from typing import Any, Dict, List, Optional, Tuple, Union
import numpy as np
import torch
from datasets import load_dataset
from jinja2 import Template
from PIL import Image
from PIL.Image import Image as ImageObject
from qwen_vl_utils.vision_process import fetch_video
from torch.utils.data import Dataset
from transformers import PreTrainedTokenizer, ProcessorMixin
from . import torch_functional as VF
from ..utils.gen_new_item import generate_variants
import json
import random
import warnings
import logging
from functools import partial
import copy

def collate_fn(features: List[Union[Dict[str, Any], List[Dict[str, Any]]]]) -> Dict[str, Any]:
    # TODO: Handle nested lists
    if isinstance(features[0], list):
        features = features[0]  # Unpack nested list

    tensors = defaultdict(list)
    non_tensors = defaultdict(list)
    for feature in features:
        for key, value in feature.items():
            if isinstance(value, torch.Tensor):
                tensors[key].append(value)
            else:
                non_tensors[key].append(value)

    for key, value in tensors.items():
        tensors[key] = torch.stack(value, dim=0)

    for key, value in non_tensors.items():
        non_tensors[key] = np.array(value, dtype=object)

    return {**tensors, **non_tensors}

# TODO: Is a new collate function needed here?
def collate_fn_DA(features: List[List[Dict[str, Any]]]) -> Dict[str, Any]:
    """
    Process results from 128 __getitem__ calls (each returning 8 samples, total 1024).
    Input features: List[List[Dict]], outer list length 128, inner list length 8.
    Output: Dict[str, Any], with tensors stacked and non-tensors as numpy arrays.
    """
    # Debug info
    print(f"Input features type: {type(features)}")
    print(f"Input features length (__getitem__ calls): {len(features)}")
    if features:
        print(f"Samples in first __getitem__: {len(features[0])}")
        print(f"First sample keys: {list(features[0][0].keys())}")

    # Flatten nested structure (128 * 8 = 1024 samples)
    flat_features = [item for sublist in features for item in sublist]
    print(f"Total flattened samples: {len(flat_features)}")

    # Separate tensor and non-tensor fields
    tensors = defaultdict(list)
    non_tensors = defaultdict(list)
    
    for feature in flat_features:
        for key, value in feature.items():
            if torch.is_tensor(value):
                tensors[key].append(value)
            else:
                non_tensors[key].append(value)
    
    # Merge tensor fields (handle variable shapes)
    for key in list(tensors.keys()):
        try:
            # Try stacking (common case)
            tensors[key] = torch.stack(tensors[key], dim=0)  # shape: [1024, ...]
        except RuntimeError:
            # Cat if shapes differ (e.g., variable length data like bounding boxes)
            tensors[key] = torch.cat(tensors[key], dim=0)
            print(f"Warning: Field {key} used torch.cat instead of stack")
    
    # Merge non-tensor fields
    for key in non_tensors:
        try:
            non_tensors[key] = np.array(non_tensors[key], dtype=object)
        except Exception as e:
            print(f"Cannot convert field {key} to numpy array: {str(e)}")
            non_tensors[key] = non_tensors[key]  # Keep as is
    
    return {**tensors, **non_tensors}

def process_image(
    image: Union[dict[str, Any], ImageObject, str], min_pixels: Optional[int], max_pixels: Optional[int]
) -> ImageObject:
    if isinstance(image, str):
        image = Image.open(image)
    elif isinstance(image, dict):
        image = Image.open(BytesIO(image["bytes"]))
    elif isinstance(image, bytes):
        image = Image.open(BytesIO(image))

    image.load()  # avoid "Too many open files" errors
    if max_pixels is not None and (image.width * image.height) > max_pixels:
        resize_factor = math.sqrt(max_pixels / (image.width * image.height))
        width, height = int(image.width * resize_factor), int(image.height * resize_factor)
        image = image.resize((width, height))

    if min_pixels is not None and (image.width * image.height) < min_pixels:
        resize_factor = math.sqrt(min_pixels / (image.width * image.height))
        width, height = int(image.width * resize_factor), int(image.height * resize_factor)
        image = image.resize((width, height))

    if image.mode != "RGB":
        image = image.convert("RGB")

    return image


def process_video(
    video: str, min_pixels: Optional[int], max_pixels: Optional[int], video_fps: float, return_fps: bool = False
) -> Union[list[ImageObject], tuple[list[ImageObject], list[float]]]:
    vision_info = {"video": video, "min_pixels": min_pixels, "max_pixels": max_pixels, "fps": video_fps}
    return fetch_video(vision_info, return_video_sample_fps=return_fps)


class RLHFDataset(Dataset):
    """
    We assume the dataset contains a column that contains prompts and other information
    """

    def __init__(
        self,
        data_path: str,
        tokenizer: PreTrainedTokenizer,
        processor: Optional[ProcessorMixin],
        prompt_key: str = "prompt",
        answer_key: str = "answer",
        image_key: str = "images",
        video_key: str = "videos",
        image_dir: Optional[str] = None,
        video_fps: float = 2.0,
        max_prompt_length: int = 1024,
        truncation: str = "error",
        format_prompt: Optional[str] = None,
        min_pixels: Optional[int] = None,
        max_pixels: Optional[int] = None,
        filter_overlong_prompts: bool = True,
        filter_overlong_prompts_workers: int = 16,
        DIVA_GRPO: Optional[bool] = False,
        Variant_Num: Optional[int] = 0,
        Dataset_Mode: Optional[str] = None
    ):
        self.tokenizer = tokenizer
        self.processor = processor
        self.prompt_key = prompt_key
        self.answer_key = answer_key
        self.image_key = image_key
        self.video_key = video_key
        self.image_dir = image_dir
        self.video_fps = video_fps
        self.max_prompt_length = max_prompt_length
        self.truncation = truncation
        self.min_pixels = min_pixels
        self.max_pixels = max_pixels
        self.DIVA_GRPO = DIVA_GRPO
        self.Variant_Num = Variant_Num
        self.Dataset_Mode = Dataset_Mode

        if "@" in data_path:
            data_path, data_split = data_path.split("@")
        else:
            data_split = "train"

        if os.path.isdir(data_path):
            # when we use dataset builder, we should always refer to the train split
            file_type = os.path.splitext(os.listdir(data_path)[0])[-1][1:].replace("jsonl", "json")
            self.dataset = load_dataset(file_type, data_dir=data_path, split=data_split)
        elif os.path.isfile(data_path):
            file_type = os.path.splitext(data_path)[-1][1:].replace("jsonl", "json")
            self.dataset = load_dataset(file_type, data_files=data_path, split=data_split)
        else:
            # load remote dataset from huggingface hub
            self.dataset = load_dataset(data_path, split=data_split)

        # TODO: Should format_prompt be set to None here?
        self.format_prompt = None
        if format_prompt:
            with open(format_prompt, encoding="utf-8") as f:
                self.format_prompt = f.read()

        if self.DIVA_GRPO == False or filter_overlong_prompts:
            print(f"Before overlong prompts filter datasets length is === {len(self.dataset)}")
            self.dataset = self.dataset.filter(
                self._filter_overlong_prompts,
                desc="Filtering overlong prompts",
                num_proc=filter_overlong_prompts_workers,
            )
            print(f"After overlong prompts filter datasets length is === {len(self.dataset)}")

    def _build_messages(self, example: dict[str, Any]) -> list[dict[str, Any]]:
        prompt_str: str = example[self.prompt_key]
        if self.format_prompt:
            format_prompt = Template(self.format_prompt.strip())
            prompt_str = format_prompt.render(content=prompt_str)

        if self.image_key in example:
            # https://huggingface.co/docs/transformers/en/tasks/image_text_to_text
            content_list = []
            # TODO: Dataset has only one image, use first split only
            parts = prompt_str.split("<image>", 1)  # Split only once
            if parts[0]:  # First part (content before <image>)
                content_list.append({"type": "text", "text": parts[0]})
            if len(parts) > 1:  # If <image> tag exists
                content_list.append({"type": "image"})
                if parts[1]:  # Content after <image>
                    content_list.append({"type": "text", "text": parts[1]})
            return [{"role": "user", "content": content_list}]
        elif self.video_key in example:
            content_list = []
            for i, content in enumerate(prompt_str.split("<video>")):
                if i != 0:
                    content_list.append({"type": "video"})

                if content:
                    content_list.append({"type": "text", "text": content})

            return [{"role": "user", "content": content_list}]
        else:
            return [{"role": "user", "content": prompt_str}]

    def _filter_overlong_prompts(self, example: dict[str, Any]) -> bool:
        messages = self._build_messages(example)
        if self.image_key in example:
            prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
            images = example[self.image_key]
            if self.image_dir is not None and len(images) != 0 and isinstance(images[0], str):  # image paths
                images = [os.path.join(self.image_dir, image) for image in images]

            processed_images = [] if len(images) != 0 else None  # text-only data
            # TODO: Add image processing code
            if isinstance(images, dict):
                processed_images.append(process_image(images, self.min_pixels, self.max_pixels))
            else:
                for image in images:
                    processed_images.append(process_image(image, self.min_pixels, self.max_pixels))

            model_inputs = self.processor(processed_images, [prompt], add_special_tokens=False, return_tensors="pt")
            return model_inputs["input_ids"].size(-1) <= self.max_prompt_length
        elif self.video_key in example:
            prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
            videos = example[self.video_key]
            if self.image_dir is not None and len(videos) != 0 and isinstance(videos[0], str):  # video paths
                videos = [os.path.join(self.image_dir, video) for video in videos]

            processed_videos = [] if len(videos) != 0 else None  # text-only data
            for video in videos:
                processed_videos.append(process_video(video, self.min_pixels, self.max_pixels, self.video_fps))

            model_inputs = self.processor(
                videos=processed_videos, text=[prompt], add_special_tokens=False, return_tensors="pt"
            )
            return model_inputs["input_ids"].size(-1) <= self.max_prompt_length
        else:
            input_ids = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True)
            return len(input_ids) <= self.max_prompt_length

    def __len__(self):
        return len(self.dataset)

    def update_difficulty(self, updates):
        """
        Batch update sample difficulty (safe and efficient version).
        
        Args:
            updates: List containing (uid, new_difficulty) tuples.
            
        Returns:
            bool: Whether all samples were successfully updated.
        """
        # Convert updates to dict for fast lookup
        uid_to_diff = {str(uid): new_diff for uid, new_diff in updates}
        print(f"\n[DEBUG] Starting difficulty update for {len(updates)} samples...")

        # Stats before update
        before_diff_dist = dict(zip(*np.unique(self.dataset['difficulty'], return_counts=True)))
        
        # Define update function
        def apply_update(example):
            uid = str(example.get('id'))
            if uid in uid_to_diff:
                example['difficulty'] = uid_to_diff[uid]
            return example

        # Execute batch update (auto-parallel)
        self.dataset = self.dataset.map(
            apply_update, num_proc=4, desc="Updating difficulties"
        )

        # Check for missing IDs
        all_ids = set(str(x) for x in self.dataset['id'])
        missing_ids = [uid for uid in uid_to_diff if uid not in all_ids]

        # Output update results
        after_diff_dist = dict(zip(*np.unique(self.dataset['difficulty'], return_counts=True)))
        print(f"\n[DEBUG] Update summary:")
        print(f"  - Total updates attempted: {len(updates)}")
        print(f"  - Successfully updated: {len(updates) - len(missing_ids)}")
        print(f"  - Not found samples: {len(missing_ids)}")
        if missing_ids:
            print(f"    - Missing IDs: {', '.join(missing_ids[:5])}{'...' if len(missing_ids) > 5 else ''}")

        # Random sample verification (optional)
        if updates and not missing_ids:
            print("\n[DEBUG] Random sample verification:")
            import random
            test_cases = random.sample(updates, min(5, len(updates)))
            for uid, expected_diff in test_cases:
                idx = self.dataset['id'].index(uid)
                actual_diff = self.dataset[idx]['difficulty']
                status = "✓" if actual_diff == expected_diff else "✗"
                print(f"  - ID {uid}: expected {expected_diff}, got {actual_diff} [{status}]")
        return not missing_ids

    def _build_variant_messages(self, example):
        """Build messages for variant examples"""
        prompt_str: str = example[self.prompt_key]
        
        if self.format_prompt:
            format_prompt = Template(self.format_prompt.strip())
            prompt_str = format_prompt.render(content=prompt_str)
        
        if self.image_key in example:
            content_list = []
            # TODO: Split image only once
            parts = prompt_str.split("<image>", 1)
            content_list = []
            if parts[0]:  # First part (content before <image>)
                content_list.append({"type": "text", "text": parts[0]})
            if len(parts) > 1:  # If <image> tag exists
                content_list.append({"type": "image"})
                if parts[1]:  # Content after <image>
                    content_list.append({"type": "text", "text": parts[1]})
            return [{"role": "user", "content": content_list}]
        elif self.video_key in example:
            content_list = []
            for i, content in enumerate(prompt_str.split("<video>")):
                if i != 0:
                    content_list.append({"type": "video"})
                if content:
                    content_list.append({"type": "text", "text": content})
            return [{"role": "user", "content": content_list}]
        else:
            return [{"role": "user", "content": prompt_str}]

    def _process_image_data(self, example, messages):
        """Process image data"""
        prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        images = example.pop(self.image_key)
        
        # Handle image paths
        if self.image_dir is not None and images and isinstance(images[0], str):
            images = [os.path.join(self.image_dir, image) for image in images]
        
        # Handle image data
        processed_images = [] if len(images) != 0 else None  # text-only data
        if isinstance(images, dict):
            processed_images.append(process_image(images, self.min_pixels, self.max_pixels))
        else:
            for image in images:
                processed_images.append(process_image(image, self.min_pixels, self.max_pixels))
        
        # Get model inputs
        model_inputs = self.processor(processed_images, [prompt], add_special_tokens=False, return_tensors="pt")
        input_ids = model_inputs.pop("input_ids")[0]
        attention_mask = model_inputs.pop("attention_mask")[0]
        
        # Save multi-modal data
        example["multi_modal_data"] = {"images": images}
        
        return self._finalize_example(example, model_inputs, input_ids, attention_mask, prompt)

    def _process_video_data(self, example, messages):
        """Process video data"""
        prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        videos = example.pop(self.video_key)
        
        # Handle video paths
        if self.image_dir is not None and videos and isinstance(videos[0], str):
            videos = [os.path.join(self.image_dir, video) for video in videos]
        
        # Handle video data
        processed_videos = [] if len(videos) != 0 else None
        video_fps_list = []
        for video in videos:
            processed_video, video_fps = process_video(
                video, self.min_pixels, self.max_pixels, self.video_fps, return_fps=True
            )
            processed_videos.append(processed_video)
            video_fps_list.append(video_fps)
        
        # Get model inputs
        model_inputs = self.processor(
            videos=processed_videos, text=[prompt], add_special_tokens=False, return_tensors="pt"
        )
        
        # Qwen2VL specific handling
        if "second_per_grid_ts" in self.processor.model_input_names:
            model_inputs["second_per_grid_ts"] = [2.0 / fps for fps in video_fps_list]
        
        input_ids = model_inputs.pop("input_ids")[0]
        attention_mask = model_inputs.pop("attention_mask")[0]
        
        # Save multi-modal data
        example["multi_modal_data"] = {"videos": videos}
        
        return self._finalize_example(example, model_inputs, input_ids, attention_mask, prompt)

    def _process_text_data(self, example, messages):
        """Process text-only data"""
        prompt = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        model_inputs = self.tokenizer([prompt], add_special_tokens=False, return_tensors="pt")
        input_ids = model_inputs.pop("input_ids")[0]
        attention_mask = model_inputs.pop("attention_mask")[0]
        
        return self._finalize_example(example, model_inputs, input_ids, attention_mask, prompt)
        
    # Generic method for single example
    def _process_single_example(self, example, is_variant=False):
        copy_example = copy.deepcopy(example)
        messages = self._build_messages(example) if not is_variant else self._build_variant_messages(example)
        if self.image_key in example:
            return self._process_image_data(example, messages)
        elif self.video_key in example:
            return self._process_video_data(example, messages)
        else:
            return self._process_text_data(example, messages)

    def __getitem__(self, index):
        example: dict = self.dataset[index]
        if self.DIVA_GRPO == True:
            if self.Dataset_Mode.startswith("one_thinking"):
                varient_list = generate_variants(example, self.Variant_Num, allow_multiple_thinking=True)
                return [self._process_single_example(new_example, is_variant=True) for new_example in varient_list]
            else:
                varient_list = generate_variants(example, self.Variant_Num, allow_multiple_thinking=False)
                return [self._process_single_example(new_example, is_variant=True) for new_example in varient_list]
        else:
            return self._process_single_example(example)

    def _finalize_example(self, example, model_inputs, input_ids, attention_mask, prompt):
        """Finalize example processing"""
        # Generate position IDs
        if self.processor is not None and "Qwen2VLImageProcessor" in self.processor.image_processor.__class__.__name__:
            # qwen-vl mrope
            if "Qwen3VLProcessor" in self.processor.__class__.__name__:
                from ..models.transformers.qwen3_vl import get_rope_index
            else:
                from ..models.transformers.qwen2_vl import get_rope_index

            vision_position_ids = get_rope_index(
                self.processor,
                input_ids=input_ids,
                image_grid_thw=model_inputs.get("image_grid_thw", None),
                video_grid_thw=model_inputs.get("video_grid_thw", None),
                second_per_grid_ts=model_inputs.get("second_per_grid_ts", None),
                attention_mask=attention_mask,
            )  # (3, seq_length)
            text_position_ids = torch.arange(len(input_ids)).unsqueeze(0)  # (1, seq_length)
            position_ids = torch.cat((text_position_ids, vision_position_ids), dim=0)  # (4, seq_length)
        else:
            position_ids = torch.clip(attention_mask.cumsum(dim=0) - 1, min=0, max=None)  # (seq_length,)
        
        # Handle overlong inputs
        input_ids, attention_mask, position_ids = VF.postprocess_data(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            max_length=self.max_prompt_length,
            pad_token_id=self.tokenizer.pad_token_id,
            left_pad=True,
            truncation=self.truncation,
        )
        
        # Process raw prompt tokens
        raw_prompt_ids = self.tokenizer.encode(prompt, add_special_tokens=False)
        if len(raw_prompt_ids) > self.max_prompt_length:
            if self.truncation == "left":
                raw_prompt_ids = raw_prompt_ids[-self.max_prompt_length :]
            elif self.truncation == "right":
                raw_prompt_ids = raw_prompt_ids[: self.max_prompt_length]
            elif self.truncation == "error":
                raise RuntimeError(f"Prompt length {len(raw_prompt_ids)} is longer than {self.max_prompt_length}.")
        
        # Assemble final example
        example["input_ids"] = input_ids
        example["attention_mask"] = attention_mask
        example["position_ids"] = position_ids
        example["raw_prompt_ids"] = raw_prompt_ids
        example["ground_truth"] = example[self.answer_key]
        
        return example